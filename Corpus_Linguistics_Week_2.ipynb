{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Corpus Linguistics Week 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew93562/MazeProject/blob/master/Corpus_Linguistics_Week_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJ8SuQbfrTR"
      },
      "source": [
        "**Welcome to Week 2!** This week are going to continue looking at text manipulations and file input/output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc1EYwINh9pU"
      },
      "source": [
        "Last week we tried to figure out ways to count the words in a text. This problem is also called **tokenization**. The **tokens** in a text is each individual word. The **types** refer to the vocabulary items in the text.\n",
        "\n",
        "*For example, \"cat cat cat\" has 3 tokens, and 1 type.\n",
        "\n",
        "The first and simplest trick for tokenization is to use Python's `split()` method with no arguments---this will magically split based on all whitespace characters, such as `\" \"`, `\"\\n\"`, \"`\\r\\n`\", and friends, plus it will also split only once if there are multiple whitespaces next to each other. So:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaQSdrd7iWUR"
      },
      "source": [
        "weird_text = \"this  is\\tweird\\r\\n text as you can\\tsee\\n\"\n",
        "print(weird_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsbfmA2tihIR"
      },
      "source": [
        "parts_of_weird_text = weird_text.split('t')\n",
        "print(parts_of_weird_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHrebbHzilkT"
      },
      "source": [
        "This nicely gets rid of a lot of weirdness and it helps us solve our problem with the Dickens text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVGfqh5_ik6I"
      },
      "source": [
        "words = text.casefold().split() #another way to do lowercase other than .lower()\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahYqCnemjDmF"
      },
      "source": [
        "Let's use split to compare two pieces of text.\n",
        "\n",
        "Take our familiar Dickens below. There's a second text very like the first, but different. Some \"the\"s are missing. Here's a way we can detect the difference between two texts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d209ozX1jn7p"
      },
      "source": [
        "text = \"\"\"It was the best of times, it was the worst of times, it was the age of\n",
        "wisdom, it was the age of foolishness, it was the epoch of belief, it was the \n",
        "epoch of incredulity, it was the season of Light, it was the season of Darkness,\n",
        "it was the spring of hope, it was the winter of despair, we had everything \n",
        "before us, we had nothing before us, we were all going direct to Heaven, we were\n",
        "all going direct the other way – in short, the period was so far like the \n",
        "present period, that some of its noisiest authorities insisted on its being \n",
        "received, for good or for evil, in the superlative degree of comparison only.\n",
        "\"\"\"\n",
        "\n",
        "text2 = \"\"\"It was best of times, it was the worst of times, it was the age of\n",
        "wisdom, it was  age of foolishness, it was the epoch of belief, it was the \n",
        "epoch of incredulity, it was the season of Light, it was the season of Darkness,\n",
        "it was the spring of hope, it was the winter of despair, we had everything \n",
        "before us, we had nothing before us, we were all going direct to Heaven, we were\n",
        "all going direct the other way – in short, the period was so far like the \n",
        "present period, that some of its noisiest authorities insisted on its being \n",
        "received, for good or for evil, in the superlative degree of comparison only.\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Ko8hXEo8jC"
      },
      "source": [
        "#list\n",
        "my_lovely_list = ['a', 'nice', 'nice', 'dog']\n",
        "print(len(my_lovely_list))\n",
        "\n",
        "my_lovely_set = set(my_lovely_list)\n",
        "print(my_lovely_set)\n",
        "\n",
        "\n",
        "#sets! \n",
        "thing1 = \"abc\"\n",
        "thing2 = \"bcd\"\n",
        "#first set - second set = the things unique to first set\n",
        "print(set(thing1) - set(thing2))\n",
        "print(set(thing2) - set(thing1))\n",
        "#first set | second set = combined sets\n",
        "print(set(thing1) | set(thing2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exaKagqiov9h"
      },
      "source": [
        "from collections import Counter #this imports a lovely lovely container named Counter.\n",
        "#surprise! It counts!\n",
        "text_count = Counter(text.lower().split())\n",
        "text2_count = Counter(text2.lower().split())\n",
        "print(text_count)\n",
        "print(text2_count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ZWYHIzmhrB"
      },
      "source": [
        "#write code here that compared the word frequency lists of text1 to text2. How many \"the\"s are missing?\n",
        "diff = text_count - text2_count\n",
        "print(diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3buowvu2jAuS"
      },
      "source": [
        "Even though we can compare wordlists this way, when we use ```split()``` the word tokens are still a little off. Do you see why? Take a look at the content of the list `words` from above and see if the tokens are all corresponding to single words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyO4dPWNi-gs"
      },
      "source": [
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzPDZzSknSPu"
      },
      "source": [
        "Maybe our first solution is to get rid of punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jquA0qFur3Mi"
      },
      "source": [
        "#get rid of puncutation\n",
        "test = 'meow   wow..!!'\n",
        "#replace \n",
        "test = test.replace('.', '') #replace . with nothing\n",
        "test = test.replace('!', '') #replace ! with nothing\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Ekff1Ll7s2"
      },
      "source": [
        "# get rid of allll punctuation muahahaha!!!\n",
        "import string\n",
        "string.punctuation\n",
        "test = 'meow   wow..!!'\n",
        "new_test = ''\n",
        "num = 0\n",
        "\n",
        "for character in test:\n",
        "  if character not in string.punctuation:\n",
        "    new_test += character\n",
        "    num += 1\n",
        "\n",
        "print(new_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_6G8pe7nxv7"
      },
      "source": [
        "# we can also get rid of that pesky extra whitespace if we want. how do we do that?\n",
        "new_test = new_test.replace('  ', '')\n",
        "print(new_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lypb1a6kmHVk"
      },
      "source": [
        "help(str.maketrans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afqeZ9ChmSR8"
      },
      "source": [
        "help(str.translate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyb6hzgemXW8"
      },
      "source": [
        "table = str.maketrans({',': ''})\n",
        "\"cats, are, here\".translate(table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt6gRrWojR6r"
      },
      "source": [
        "The remaining problem is that many of the word tokens that come out this way have punctuation attached to them. If we are interested in the meaningful words in a text, this is a problem. We don't want to think of `incredulity,` and `incredulity` as two different words. If someone asked \"does this text contain the word incredulity?\" and you answer no, it contains the word `incredulity,`, then you will not be giving the right answer.\n",
        "\n",
        "One solution is to separate all the punctuation out into separate tokens. On the other hand, sometimes this may not be desirable: for example, it might make sense to keep the `.` attached to abbreviations like `Mr.`. Ultimately, your choice depends on what you will be doing with the word tokens later on.\n",
        "\n",
        "Tokenization is the first hard problem in corpus linguistics. Now I'd like you to try to write a tokenizer: a function that takes in a string, and outputs a list of the *meaningful* word tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSYfcQ4coS2W"
      },
      "source": [
        "***Luckily, there are easy ways to address this using built in packages like ```nltk```. We'll look at the fancy things we can do with this package next time.***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9LntZFHgBTA"
      },
      "source": [
        "Now that we can do cruel and unusual things to text, let's grap a file from the web.\n",
        "\n",
        "Once we do that, put it in the same google drive folder your copy of this notebook is in.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNAlZA6ylESU"
      },
      "source": [
        "Now that we have a file on our computer, let's import it \n",
        "If you are unable to access the server, you can copy\n",
        "[Jane Austen's \"Emma\"](https://drive.google.com/file/d/1O8YDZpxUOZ4kdvhxuoYOwkbjhDczmvRE/view?usp=sharing). Make a copy in the same drive folder you have this notebook in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dq-IoJili1j"
      },
      "source": [
        "#to import a file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go97f52pnqwI"
      },
      "source": [
        "infile = open('/content/drive/My Drive/Pomona materials/Corpus Linguistics Fall 2020/austen-emma.txt', 'r')\n",
        "readinfile = infile.read()\n",
        "print(readinfile[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQakbaXco_Gk"
      },
      "source": [
        "**Exercises:**\n",
        "\n",
        "1. Only one author in /data/corpora/Gutenberg-1small uses the word 'and' more often than 'the'. Who is it?"
      ]
    }
  ]
}